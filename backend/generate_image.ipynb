{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'model' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Initialize your model\u001b[39;00m\n\u001b[0;32m    101\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Use CPU\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m your_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_your_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Example of how to use your generator\u001b[39;00m\n\u001b[0;32m    105\u001b[0m ngf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "Cell \u001b[1;32mIn[24], line 11\u001b[0m, in \u001b[0;36mload_your_trained_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_your_trained_model\u001b[39m():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Replace with actual model loading logic\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Example\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend/state_epoch_001_180-indowestern.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'model' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Assuming you have a method to load your model weights\n",
    "def load_your_trained_model():\n",
    "    # Replace with actual model loading logic\n",
    "    # Example\n",
    "    model=model.load_state_dict(torch.load('backend/state_epoch_001_180-indowestern.pth', map_location='cpu'))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "class G_Block(nn.Module):\n",
    "    def __init__(self, cond_dim, in_ch, out_ch, imsize):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, h, y):\n",
    "        h = self.fuse1(h, y)\n",
    "        h = self.c1(h)\n",
    "        h = self.fuse2(h, y)\n",
    "        h = self.c2(h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, h, y):\n",
    "        h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        return self.shortcut(h) + self.residual(h, y)\n",
    "\n",
    "# Define other blocks as needed (D_Block, M_Block, etc.)\n",
    "\n",
    "class CLIP_Adapter(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        self.FBlocks = nn.ModuleList([])\n",
    "        self.FBlocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for i in range(map_num - 1):\n",
    "            self.FBlocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        self.conv_fuse = nn.Conv2d(out_ch, CLIP_ch, 5, 1, 2)\n",
    "        self.conv = nn.Conv2d(768, G_ch, 5, 1, 2)\n",
    "\n",
    "    def forward(self, out, c):\n",
    "        for FBlock in self.FBlocks:\n",
    "            out = FBlock(out, c)\n",
    "        fuse_feat = self.conv_fuse(out)\n",
    "        return self.conv(fuse_feat)\n",
    "\n",
    "class NetG(nn.Module):\n",
    "    def __init__(self, ngf, nz, cond_dim, imsize, ch_size, mixed_precision):\n",
    "        super(NetG, self).__init__()\n",
    "        self.ngf = ngf\n",
    "        self.code_sz, self.code_ch, self.mid_ch = 7, 64, 32\n",
    "        self.fc_code = nn.Linear(nz, self.code_sz * self.code_sz * self.code_ch)\n",
    "        self.mapping = CLIP_Adapter(self.code_ch, self.mid_ch, self.code_ch, ngf * 8, 768, cond_dim + nz, 3, 1, 1, 4)\n",
    "        self.GBlocks = nn.ModuleList([])\n",
    "        in_out_pairs = list(get_G_in_out_chs(ngf, imsize))\n",
    "        imsize = 4\n",
    "        for idx, (in_ch, out_ch) in enumerate(in_out_pairs):\n",
    "            if idx < (len(in_out_pairs) - 1):\n",
    "                imsize = imsize * 2\n",
    "            else:\n",
    "                imsize = 224\n",
    "            self.GBlocks.append(G_Block(cond_dim + nz, in_ch, out_ch, imsize))\n",
    "        self.to_rgb = nn.Sequential(\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_ch, ch_size, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, c, eval=False):\n",
    "        cond = torch.cat((noise, c), dim=1)\n",
    "        out = self.mapping(self.fc_code(noise).view(noise.size(0), self.code_ch, self.code_sz, self.code_sz), cond)\n",
    "        for GBlock in self.GBlocks:\n",
    "            out = GBlock(out, cond)\n",
    "        out = self.to_rgb(out)\n",
    "        return out\n",
    "\n",
    "# Define your NetD and NetC similarly\n",
    "\n",
    "def get_G_in_out_chs(nf, imsize):\n",
    "    layer_num = int(np.log2(imsize)) - 1\n",
    "    channel_nums = [nf * min(2 ** idx, 8) for idx in range(layer_num)]\n",
    "    channel_nums = channel_nums[::-1]\n",
    "    in_out_pairs = zip(channel_nums[:-1], channel_nums[1:])\n",
    "    return in_out_pairs\n",
    "\n",
    "# Initialize your model\n",
    "device = 'cpu'  # Use CPU\n",
    "your_model = load_your_trained_model().to(device)\n",
    "\n",
    "# Example of how to use your generator\n",
    "ngf = 64\n",
    "nz = 100\n",
    "cond_dim = 10\n",
    "imsize = 224\n",
    "ch_size = 3\n",
    "mixed_precision = False\n",
    "\n",
    "netG = NetG(ngf, nz, cond_dim, imsize, ch_size, mixed_precision).to(device)\n",
    "\n",
    "# Load the trained model weights into your generator\n",
    "netG.load_state_dict(your_model.state_dict())  # Assuming your_model is compatible\n",
    "\n",
    "# Example input data\n",
    "noise = torch.randn(1, nz).to(device)\n",
    "cond = torch.randn(1, cond_dim).to(device)\n",
    "\n",
    "# Generate output\n",
    "output = netG(noise, cond)\n",
    "\n",
    "print(output.shape)  # Check output shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'visual'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_text_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m CLIPModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m galip_obj \u001b[38;5;241m=\u001b[39m \u001b[43mGalipModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     27\u001b[0m galip_obj\u001b[38;5;241m.\u001b[39mnetG\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[19], line 16\u001b[0m, in \u001b[0;36mGalipModel.__init__\u001b[1;34m(self, ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetG \u001b[38;5;241m=\u001b[39m \u001b[43mNetG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mngf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixed_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 243\u001b[0m, in \u001b[0;36mNetG.__init__\u001b[1;34m(self, ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLIP_ch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_code \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(nz,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_sz\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_sz\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode_ch)\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapping \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP_Adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngf\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIP_ch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_dim\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGBlocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([])\n\u001b[0;32m    245\u001b[0m in_out_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(get_G_in_out_chs(ngf, imsize))\n",
      "Cell \u001b[1;32mIn[18], line 223\u001b[0m, in \u001b[0;36mCLIP_Adapter.__init__\u001b[1;34m(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, CLIP)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFBlocks\u001b[38;5;241m.\u001b[39mappend(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_fuse \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(out_ch, CLIP_ch, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCLIP_ViT \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP_Mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(\u001b[38;5;241m768\u001b[39m, G_ch, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 168\u001b[0m, in \u001b[0;36mCLIP_Mapper.__init__\u001b[1;34m(self, CLIP)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,CLIP):\n\u001b[0;32m    167\u001b[0m   \u001b[38;5;28msuper\u001b[39m(CLIP_Mapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 168\u001b[0m   model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\n\u001b[0;32m    169\u001b[0m   \u001b[38;5;66;03m# print(model)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefine_module(model)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'visual'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from argparse import Namespace\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from torch import distributed as dist\n",
    "from transformers import CLIPTokenizer\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from torch.cuda.amp import autocast\n",
    "from scipy import linalg\n",
    "\n",
    "\n",
    "class CLIP_IMG_ENCODER(nn.Module):\n",
    "    def __init__(self, CLIP):\n",
    "        super(CLIP_IMG_ENCODER, self).__init__()\n",
    "        model = CLIP.visual\n",
    "        # print(model)\n",
    "        self.define_module(model)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def define_module(self, model):\n",
    "        self.conv1 = model.conv1\n",
    "        self.class_embedding = model.class_embedding\n",
    "        self.positional_embedding = model.positional_embedding\n",
    "        self.ln_pre = model.ln_pre\n",
    "        self.transformer = model.transformer\n",
    "        self.ln_post = model.ln_post\n",
    "        self.proj = model.proj\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.conv1.weight.dtype\n",
    "\n",
    "    def transf_to_CLIP_input(self,inputs):\n",
    "        device = inputs.device\n",
    "        if len(inputs.size()) != 4:\n",
    "            raise ValueError('Expect the (B, C, X, Y) tensor.')\n",
    "        else:\n",
    "            mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\\\n",
    "                .unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "            var = torch.tensor([0.26862954, 0.26130258, 0.27577711])\\\n",
    "                .unsqueeze(-1).unsqueeze(-1).unsqueeze(0).to(device)\n",
    "            inputs = F.interpolate(inputs*0.5+0.5, size=(224, 224))\n",
    "            inputs = ((inputs+1)*0.5-mean)/var\n",
    "            return inputs\n",
    "\n",
    "    def forward(self, img: torch.Tensor):\n",
    "        x = self.transf_to_CLIP_input(img)\n",
    "        x = x.type(self.dtype)\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        grid =  x.size(-1)\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "        # NLD -> LND\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # Local features\n",
    "        selected = [1,4,8]\n",
    "#         selected = [2,3,4,6,8,10]\n",
    "        local_features = []\n",
    "        for i in range(12):\n",
    "            x = self.transformer.resblocks[i](x)\n",
    "            if i in selected:\n",
    "                local_features.append(x.permute(1, 0, 2)[:, 1:, :].permute(0, 2, 1).reshape(-1, 768, grid, grid).contiguous().type(img.dtype))\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "        return torch.stack(local_features, dim=1), x.type(img.dtype)\n",
    "\n",
    "\n",
    "class CLIP_TXT_ENCODER(nn.Module):\n",
    "    def __init__(self, CLIP):\n",
    "        super(CLIP_TXT_ENCODER, self).__init__()\n",
    "        self.define_module(CLIP)\n",
    "        # print(model)\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def define_module(self, CLIP):\n",
    "        self.transformer = CLIP.transformer\n",
    "        self.vocab_size = CLIP.vocab_size\n",
    "        self.token_embedding = CLIP.token_embedding\n",
    "        self.positional_embedding = CLIP.positional_embedding\n",
    "        self.ln_final = CLIP.ln_final\n",
    "        self.text_projection = CLIP.text_projection\n",
    "        self.context_length = self.positional_embedding.shape[0]\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.transformer.resblocks[0].mlp.c_fc.weight.dtype\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        sent_emb = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "        return sent_emb, x\n",
    "    \n",
    "\n",
    "class G_Block(nn.Module):\n",
    "    def __init__(self, cond_dim, in_ch, out_ch, imsize):\n",
    "        super(G_Block, self).__init__()\n",
    "        self.imsize = imsize\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n",
    "        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n",
    "        self.fuse1 = DFBLK(cond_dim, in_ch)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, h, y):\n",
    "        h = self.fuse1(h, y)\n",
    "        h = self.c1(h)\n",
    "        h = self.fuse2(h, y)\n",
    "        h = self.c2(h)\n",
    "        return h\n",
    "\n",
    "    def forward(self, h, y):\n",
    "        h = F.interpolate(h, size=(self.imsize, self.imsize))\n",
    "        return self.shortcut(h) + self.residual(h, y)\n",
    "\n",
    "\n",
    "class D_Block(nn.Module):\n",
    "    def __init__(self, fin, fout, k, s, p, res, CLIP_feat):\n",
    "        super(D_Block, self).__init__()\n",
    "        self.res, self.CLIP_feat = res, CLIP_feat\n",
    "        self.learned_shortcut = (fin != fout)\n",
    "        self.conv_r = nn.Sequential(\n",
    "            nn.Conv2d(fin, fout, k, s, p, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(fout, fout, k, s, p, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        self.conv_s = nn.Conv2d(fin, fout, 1, stride=1, padding=0)\n",
    "        if self.res==True:\n",
    "            self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        if self.CLIP_feat==True:\n",
    "            self.beta = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, CLIP_feat=None):\n",
    "        res = self.conv_r(x)\n",
    "        if self.learned_shortcut:\n",
    "            x = self.conv_s(x)\n",
    "        \n",
    "        if (self.res==True)and(self.CLIP_feat==True):\n",
    "            return x + self.gamma*res + self.beta*CLIP_feat\n",
    "        elif (self.res==True)and(self.CLIP_feat!=True):\n",
    "            return x + self.gamma*res\n",
    "        elif (self.res!=True)and(self.CLIP_feat==True):\n",
    "            return x + self.beta*CLIP_feat\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "\n",
    "class Affine(nn.Module):\n",
    "    def __init__(self, cond_dim, num_features):\n",
    "        super(Affine, self).__init__()\n",
    "\n",
    "        self.fc_gamma = nn.Sequential(OrderedDict([\n",
    "            ('linear1',nn.Linear(cond_dim, num_features)),\n",
    "            ('relu1',nn.ReLU(inplace=True)),\n",
    "            ('linear2',nn.Linear(num_features, num_features)),\n",
    "            ]))\n",
    "        self.fc_beta = nn.Sequential(OrderedDict([\n",
    "            ('linear1',nn.Linear(cond_dim, num_features)),\n",
    "            ('relu1',nn.ReLU(inplace=True)),\n",
    "            ('linear2',nn.Linear(num_features, num_features)),\n",
    "            ]))\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n",
    "        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n",
    "        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n",
    "        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        weight = self.fc_gamma(y)\n",
    "        bias = self.fc_beta(y)\n",
    "\n",
    "        if weight.dim() == 1:\n",
    "            weight = weight.unsqueeze(0)\n",
    "        if bias.dim() == 1:\n",
    "            bias = bias.unsqueeze(0)\n",
    "\n",
    "        size = x.size()\n",
    "        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n",
    "        return weight * x + bias\n",
    "\n",
    "\n",
    "class DFBLK(nn.Module):\n",
    "    def __init__(self, cond_dim, in_ch):\n",
    "        super(DFBLK, self).__init__()\n",
    "        self.affine0 = Affine(cond_dim, in_ch)\n",
    "        self.affine1 = Affine(cond_dim, in_ch)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.affine0(x, y)\n",
    "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
    "        h = self.affine1(h, y)\n",
    "        h = nn.LeakyReLU(0.2,inplace=True)(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class M_Block(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, cond_dim, k, s, p):\n",
    "        super(M_Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, mid_ch, k, s, p)\n",
    "        self.fuse1 = DFBLK(cond_dim, mid_ch)\n",
    "        self.conv2 = nn.Conv2d(mid_ch, out_ch, k, s, p)\n",
    "        self.fuse2 = DFBLK(cond_dim, out_ch)\n",
    "        self.learnable_sc = in_ch != out_ch\n",
    "        if self.learnable_sc:\n",
    "            self.c_sc = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "\n",
    "    def shortcut(self, x):\n",
    "        if self.learnable_sc:\n",
    "            x = self.c_sc(x)\n",
    "        return x\n",
    "\n",
    "    def residual(self, h, text):\n",
    "        h = self.conv1(h)\n",
    "        h = self.fuse1(h, text)\n",
    "        h = self.conv2(h)\n",
    "        h = self.fuse2(h, text)\n",
    "        return h\n",
    "\n",
    "    def forward(self, h, c):\n",
    "        return self.shortcut(h) + self.residual(h, c)\n",
    "\n",
    "class CLIP_Mapper(nn.Module):\n",
    "  def __init__(self,CLIP):\n",
    "    super(CLIP_Mapper, self).__init__()\n",
    "    model = CLIP.visual\n",
    "    # print(model)\n",
    "    self.define_module(model)\n",
    "    for param in self.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "  def define_module(self,model):\n",
    "    self.conv1 = model.conv1\n",
    "    self.class_embedding = model.class_embedding\n",
    "    self.positional_embedding = model.positional_embedding\n",
    "    self.ln_pre = model.ln_pre\n",
    "    self.transformer = model.transformer\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    return self.conv1.weight.dtype\n",
    "\n",
    "\n",
    "  def forward(self,img:torch.Tensor,prompts:torch.Tensor):\n",
    "    x = img.type(self.dtype)\n",
    "    prompts = prompts.type(self.dtype)\n",
    "    grid = x.size(-1)\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "    x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "    x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
    "    # shape = [*, grid ** 2 + 1, width]\n",
    "    x = x + self.positional_embedding.to(x.dtype)\n",
    "    x = self.ln_pre(x)\n",
    "    # NLD -> LND\n",
    "    x = x.permute(1, 0, 2)\n",
    "    # Local features\n",
    "    selected = [1,4,7,12]\n",
    "#     selected = [1,2,3,4,5,6,7,8]\n",
    "    begin,end = 0 , 12\n",
    "    prompt_idx = 0\n",
    "    for i in range(begin, end):\n",
    "            if i in selected:\n",
    "                prompt = prompts[:,prompt_idx,:].unsqueeze(0)\n",
    "                prompt_idx = prompt_idx+1\n",
    "                x = torch.cat((x,prompt), dim=0)\n",
    "                x = self.transformer.resblocks[i](x)\n",
    "                x = x[:-1,:,:]\n",
    "            else:\n",
    "                x = self.transformer.resblocks[i](x)\n",
    "    return x.permute(1, 0, 2)[:, 1:, :].permute(0, 2, 1).reshape(-1, 768, grid, grid).contiguous().type(img.dtype)\n",
    "\n",
    "class CLIP_Adapter(nn.Module):\n",
    "    def __init__(self, in_ch, mid_ch, out_ch, G_ch, CLIP_ch, cond_dim, k, s, p, map_num, CLIP):\n",
    "        super(CLIP_Adapter, self).__init__()\n",
    "        self.CLIP_ch = CLIP_ch\n",
    "        self.FBlocks = nn.ModuleList([])\n",
    "        self.FBlocks.append(M_Block(in_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        for i in range(map_num-1):\n",
    "            self.FBlocks.append(M_Block(out_ch, mid_ch, out_ch, cond_dim, k, s, p))\n",
    "        self.conv_fuse = nn.Conv2d(out_ch, CLIP_ch, 5, 1, 2)\n",
    "        self.CLIP_ViT = CLIP_Mapper(CLIP)\n",
    "        self.conv = nn.Conv2d(768, G_ch, 5, 1, 2)\n",
    "        #\n",
    "        self.fc_prompt = nn.Linear(cond_dim, CLIP_ch*8)\n",
    "\n",
    "    def forward(self,out,c):\n",
    "        prompts = self.fc_prompt(c).view(c.size(0),-1,self.CLIP_ch)\n",
    "        for FBlock in self.FBlocks:\n",
    "            out = FBlock(out,c)\n",
    "        fuse_feat = self.conv_fuse(out)\n",
    "        map_feat = self.CLIP_ViT(fuse_feat,prompts)\n",
    "        return self.conv(fuse_feat+0.1*map_feat)\n",
    "class NetG(nn.Module):\n",
    "  def __init__(self,ngf,nz,cond_dim,imsize,ch_size,mixed_precision,CLIP):\n",
    "    super(NetG, self).__init__()\n",
    "    self.ngf = ngf\n",
    "    self.mixed_precision = mixed_precision\n",
    "    self.code_sz,self.code_ch,self.mid_ch = 7,64,32\n",
    "    self.CLIP_ch = 768\n",
    "    self.fc_code = nn.Linear(nz,self.code_sz*self.code_sz*self.code_ch)\n",
    "    self.mapping = CLIP_Adapter(self.code_ch, self.mid_ch, self.code_ch, ngf*8, self.CLIP_ch, cond_dim+nz, 3, 1, 1, 4, CLIP)\n",
    "    self.GBlocks = nn.ModuleList([])\n",
    "    in_out_pairs = list(get_G_in_out_chs(ngf, imsize))\n",
    "    imsize = 4\n",
    "    for idx, (in_ch, out_ch) in enumerate(in_out_pairs):\n",
    "        if idx<(len(in_out_pairs)-1):\n",
    "            imsize = imsize*2\n",
    "        else:\n",
    "            imsize = 224\n",
    "        self.GBlocks.append(G_Block(cond_dim+nz, in_ch, out_ch, imsize))\n",
    "    self.to_rgb = nn.Sequential(\n",
    "        nn.LeakyReLU(0.2,inplace=True),\n",
    "        nn.Conv2d(out_ch, ch_size, 3, 1, 1),\n",
    "    )\n",
    "\n",
    "  def forward(self, noise, c, eval=False):\n",
    "    with torch.cuda.amp.autocast() if self.mixed_precision and not eval else dummy_context_mgr() as mp:\n",
    "      cond = torch.cat((noise, c), dim=1)\n",
    "      out = self.mapping(self.fc_code(noise).view(noise.size(0), self.code_ch, self.code_sz, self.code_sz), cond)\n",
    "      # fuse text and visual features\n",
    "      for GBlock in self.GBlocks:\n",
    "          out = GBlock(out, cond)\n",
    "      # convert to RGB image\n",
    "      out = self.to_rgb(out)\n",
    "    return out\n",
    "class NetD(nn.Module):\n",
    "    def __init__(self, ndf, imsize, ch_size, mixed_precision):\n",
    "        super(NetD, self).__init__()\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.DBlocks = nn.ModuleList([\n",
    "            D_Block(768, 768, 3, 1, 1, res=True, CLIP_feat=True),\n",
    "            D_Block(768, 768, 3, 1, 1, res=True, CLIP_feat=True),\n",
    "        ])\n",
    "        self.main = D_Block(768, 512, 3, 1, 1, res=True, CLIP_feat=False)\n",
    "\n",
    "    def forward(self, h):\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision else dummy_context_mgr() as mpc:\n",
    "            out = h[:,0]\n",
    "            for idx in range(len(self.DBlocks)):\n",
    "                out = self.DBlocks[idx](out, h[:,idx+1])\n",
    "            out = self.main(out)\n",
    "        return out\n",
    "class NetC(nn.Module):\n",
    "    def __init__(self, ndf, cond_dim, mixed_precision):\n",
    "        super(NetC, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.mixed_precision = mixed_precision\n",
    "        self.joint_conv = nn.Sequential(\n",
    "            nn.Conv2d(512+512, 128, 4, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 1, 4, 1, 0, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, out, cond):\n",
    "        with torch.cuda.amp.autocast() if self.mixed_precision else dummy_context_mgr() as mpc:\n",
    "            cond = cond.view(-1, self.cond_dim, 1, 1)\n",
    "            cond = cond.repeat(1, 1, 7, 7)\n",
    "            h_c_code = torch.cat((out, cond), 1)\n",
    "            out = self.joint_conv(h_c_code)\n",
    "        return out\n",
    "def get_G_in_out_chs(nf, imsize):\n",
    "    layer_num = int(np.log2(imsize))-1\n",
    "    channel_nums = [nf*min(2**idx, 8) for idx in range(layer_num)]\n",
    "    channel_nums = channel_nums[::-1]\n",
    "    in_out_pairs = zip(channel_nums[:-1], channel_nums[1:])\n",
    "    return in_out_pairs\n",
    "\n",
    "\n",
    "def get_D_in_out_chs(nf, imsize):\n",
    "    layer_num = int(np.log2(imsize))-1\n",
    "    channel_nums = [nf*min(2**idx, 8) for idx in range(layer_num)]\n",
    "    in_out_pairs = zip(channel_nums[:-1], channel_nums[1:])\n",
    "    return in_out_pairs\n",
    "class CLIP_Model:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            # Instantiate the CLIP model only once\n",
    "            cls._instance = super(CLIP_Model, cls).__new__(cls)\n",
    "            cls.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            cls.model, cls.preprocessor = clip.load(\"ViT-B/32\", device=cls.device)\n",
    "        return cls._instance\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        \"\"\"Returns the CLIP model and preprocessor.\"\"\"\n",
    "        if cls._instance is None:\n",
    "            cls()  # Ensure instance is created\n",
    "        return cls._instance.model, cls._instance.preprocessor\n",
    "\n",
    "class GALIP_CONTAINER:\n",
    "    def __init__(self, netG, netD, netC, text_encoder, image_encoder, optimizerG, optimizerD, scaler_G, scaler_D, device):\n",
    "        self.netG = netG\n",
    "        self.netD = netD\n",
    "        self.netC = netC\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.optimizerG = optimizerG\n",
    "        self.optimizerD = optimizerD\n",
    "        self.scaler_G = scaler_G\n",
    "        self.scaler_D = scaler_D\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "def initialize_gan_model(CLIP, \n",
    "                              ngf=64, \n",
    "                              nz=100, \n",
    "                              cond_dim=512, \n",
    "                              imsize=224, \n",
    "                              ch_size=3, \n",
    "                              mixed_precision=True, \n",
    "                              ndf=64, \n",
    "                              lr_g=0.0001, \n",
    "                              lr_d=0.0004, \n",
    "                              betas=(0.0, 0.9)) -> GALIP_CONTAINER:\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize models\n",
    "    if torch.cuda.is_available():\n",
    "        netG = nn.DataParallel(NetG(ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP))\n",
    "        netD = nn.DataParallel(NetD(ndf, imsize, ch_size, mixed_precision))\n",
    "        netC = nn.DataParallel(NetC(ndf, cond_dim, mixed_precision))\n",
    "    else:\n",
    "        netG = NetG(ngf, nz, cond_dim, imsize, ch_size, mixed_precision, CLIP)\n",
    "        netD = NetD(ndf, imsize, ch_size, mixed_precision)\n",
    "        netC = NetC(ndf, cond_dim, mixed_precision)\n",
    "    \n",
    "    # Initialize text and image encoders\n",
    "    text_encoder = CLIP_TXT_ENCODER(CLIP)\n",
    "    image_encoder = CLIP_IMG_ENCODER(CLIP)\n",
    "    \n",
    "    # Move models to device and set precision\n",
    "    netG.float().to(device)\n",
    "    netD.float().to(device)\n",
    "    netC.float().to(device)\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    optimizerG = torch.optim.Adam(netG.parameters(), lr=lr_g, betas=betas)\n",
    "    optimizerD = torch.optim.Adam(netD.parameters(), lr=lr_d, betas=betas)\n",
    "    \n",
    "    # Initialize scalers for mixed precision training\n",
    "    scaler_G = torch.cuda.amp.GradScaler()\n",
    "    scaler_D = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Return the models, encoders, optimizers, and scalers\n",
    "    return GALIP_CONTAINER (\n",
    "        netG,\n",
    "        netD,\n",
    "        netC,\n",
    "        text_encoder,\n",
    "        image_encoder,\n",
    "        optimizerG,\n",
    "        optimizerD,\n",
    "        scaler_G,\n",
    "        scaler_D,\n",
    "        device\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "CLIP,_ = CLIP_Model.get_model()  # Assuming CLIP model is defined elsewhere\n",
    "galip_container = initialize_gan_model(CLIP)\n",
    "def load_model_galip(components, load_path):\n",
    "    checkpoint = torch.load(load_path, map_location=torch.device(components.device))\n",
    "    \n",
    "    # Loading model components\n",
    "    components.netG.load_state_dict(checkpoint['model']['netG'], strict=False)\n",
    "    components.netD.load_state_dict(checkpoint['model']['netD'], strict=False)\n",
    "    components.netC.load_state_dict(checkpoint['model']['netC'], strict=False)\n",
    "    components.text_encoder.load_state_dict(checkpoint['model']['text_encoder'], strict=False)\n",
    "    components.image_encoder.load_state_dict(checkpoint['model']['image_encoder'], strict=False)\n",
    "\n",
    "    # Loading optimizers\n",
    "    components.optimizerG.load_state_dict(checkpoint['optimizers']['optimizer_G'])\n",
    "    components.optimizerD.load_state_dict(checkpoint['optimizers']['optimizer_D'])\n",
    "    \n",
    "    return checkpoint['epoch']\n",
    "\n",
    "epoch = load_model_galip(galip_container, 'C:\\\\Users\\\\ADMIN\\\\Downloads\\\\state_epoch_001_180-indowestern.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
